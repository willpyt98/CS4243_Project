{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "import imageio\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image,ImageReadMode\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from skimage.transform import resize\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_label_loader(path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "     path: Folder path containing subfolders of images\n",
    "    Output:\n",
    "     images: List of image path\n",
    "     labels: Numpy array of labels\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for s_folder in os.listdir(path):\n",
    "        label = int(''.join([s for s in s_folder if s.isdigit()]))\n",
    "        img_folder = os.path.join(path, s_folder)\n",
    "        \n",
    "        for img in os.listdir(img_folder):\n",
    "            if img.endswith(\".jpg\"):\n",
    "                image_path = os.path.join(img_folder, img)\n",
    "                images.append(image_path)\n",
    "                labels.append(label)\n",
    "                \n",
    "    labels = np.array(labels) - 1\n",
    "    images = np.array(images)\n",
    "                \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir,labels, transform = False):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_dir)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_dir[idx]\n",
    "        img = read_image(img_path, ImageReadMode.RGB).float()\n",
    "        if self.transform != False:\n",
    "            img = self.transform(img)\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(img_path, img_label, batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size = (128,128)),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "        \n",
    "    \n",
    "    dataset = CustomImageDataset(img_path, img_label, transform)\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test_set(net, test_loader,is_cnn = True, verbose=1):\n",
    "    \n",
    "    total_error = 0\n",
    "    batch_num = 0\n",
    "    \n",
    "    for test_data, test_label in test_loader:\n",
    "        \n",
    "        bs = test_label.shape[0]\n",
    "        test_data=test_data.to(device)\n",
    "        test_label=test_label.to(device)\n",
    "        \n",
    "        if is_cnn:\n",
    "            inputs = test_data\n",
    "        else:\n",
    "            inputs = test_data.view(bs, 128*128*3)\n",
    "        \n",
    "        scores=net( inputs ) \n",
    "\n",
    "        error = utils.get_error( scores , test_label)\n",
    "\n",
    "        total_error += error.item()\n",
    "        \n",
    "        batch_num += 1\n",
    "        \n",
    "    total_error = total_error / batch_num\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print( 'error rate on test set =', total_error*100 ,'percent\\n')\n",
    "        \n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, n_epoch, my_lr, train_loader, test_loader, is_cnn = True, momentum=0.9, verbose=1):\n",
    "    \"\"\"\n",
    "    Train a given model with specified hyperparameters\n",
    "    \n",
    "    Args:\n",
    "     net: NN model to be trained\n",
    "     n_epoch: Number of epochs to train the model\n",
    "     bs: Batch size for minibatch GD\n",
    "     lr: Learning rate of GD\n",
    "     train_data: Torch tensor of dim [N:rgb:width:height]\n",
    "     train_label: Torch tensor of dim [N]\n",
    "     verbose: Print out metrics during training if 1, default 1\n",
    "    \n",
    "    Output:\n",
    "     net: Trained NN model\n",
    "     records: Dictionary containing metrics history, including training loss/error for each epoch/minibatch, test error for each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(train_loader)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    \n",
    "    train_loss_hist_mb = []\n",
    "    train_loss_hist = []\n",
    "    train_error_hist = []\n",
    "    test_error_hist = []\n",
    "    \n",
    "    start=time.time()\n",
    "\n",
    "    for epoch in range(1,n_epoch+1):\n",
    "\n",
    "        if not epoch%5:\n",
    "            my_lr = my_lr / 1.5\n",
    "\n",
    "        # optimizer=torch.optim.SGD( net.parameters() , lr=my_lr, momentum=momentum )\n",
    "        optimizer=torch.optim.Adam(net.parameters(), lr=my_lr)\n",
    "\n",
    "        running_loss=0\n",
    "        running_error=0\n",
    "        num_batches=0\n",
    "            \n",
    "        for minibatch_data, minibatch_label in train_loader:\n",
    "\n",
    "\n",
    "            # FORWARD AND BACKWARD PASS\n",
    "            bs = minibatch_label.shape[0]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            minibatch_data=minibatch_data.to(device)                \n",
    "            minibatch_label=minibatch_label.to(device)\n",
    "\n",
    "            if is_cnn:\n",
    "                inputs = minibatch_data\n",
    "            else:                    \n",
    "                inputs = minibatch_data.view(bs, 128*128*3)\n",
    "\n",
    "\n",
    "            inputs.requires_grad_()\n",
    "\n",
    "            scores=net( inputs ) \n",
    "\n",
    "            loss =  criterion( scores , minibatch_label.long()) \n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            # COMPUTE STATS\n",
    "            running_loss += loss.detach().item()\n",
    "\n",
    "            error = utils.get_error( scores.detach() , minibatch_label)\n",
    "            running_error += error.item()\n",
    "\n",
    "            num_batches+=1   \n",
    "            train_loss_hist_mb.append(running_error/num_batches)\n",
    "                \n",
    "        # AVERAGE STATS THEN DISPLAY\n",
    "        total_loss = running_loss/num_batches\n",
    "        total_error = running_error/num_batches\n",
    "        elapsed = (time.time()-start)/60\n",
    "\n",
    "        if verbose == 1:\n",
    "            print('epoch=',epoch, '\\t time=', elapsed,'min', '\\t lr=', my_lr  ,'\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "        test_error = eval_on_test_set(net, test_loader,is_cnn = is_cnn, verbose=verbose) \n",
    "        \n",
    "        train_loss_hist.append(total_loss)\n",
    "        train_error_hist.append(total_error)\n",
    "        test_error_hist.append(test_error)\n",
    "    \n",
    "    records = {'train_loss_mb': train_loss_hist_mb,\n",
    "              'train_loss': train_loss_hist,\n",
    "              'train_error': train_error_hist,\n",
    "              'test_error': test_error_hist}\n",
    "        \n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2,  output_size):\n",
    "        super(mlp , self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(  input_size   , hidden_size1)\n",
    "        self.layer2 = nn.Linear(  hidden_size1 , hidden_size2)\n",
    "        self.layer3 = nn.Linear(  hidden_size2 , output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y       = self.layer1(x)\n",
    "        y_hat   = torch.relu(y)\n",
    "        z       = self.layer2(y_hat)\n",
    "        z_hat   = torch.relu(z)\n",
    "        scores  = self.layer3(z_hat)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5_convnet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(LeNet5_convnet, self).__init__()\n",
    "        \n",
    "        self.layer = nn.Sequential(\n",
    "            # CL1:   3 * 128 x 128  -->    50 x 128 x 128 \n",
    "            nn.Conv2d(3,   50,  kernel_size=3,  padding=1 ),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # MP1: 50 x 128 x 128 -->    50 x 64 x 64\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            # CL2:   50 x 64 x 64  -->    100 x 64 x 64 \n",
    "            nn.Conv2d(50,  100,  kernel_size=3,  padding=1 ),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # MP2: 100 x 64 x 64 -->    100 x 32 x 32\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            # MP3: 100 * 32 * 32 -> 100 * 32 * 32 (Grad needed for later CAM)\n",
    "            nn.Conv2d(100,100, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            # LL1:   100 x 32 x 32 = 102400 -->  100 \n",
    "            nn.Linear(102400, 100),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # LL2:   100  -->  13 \n",
    "            nn.Linear(100,13),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer(x)\n",
    "        out = out.view(-1, 102400)\n",
    "        out = self.fc_layer(out)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(VGG19, self).__init__()\n",
    "        \n",
    "        self.layer = nn.Sequential(\n",
    "            # CL1:   3 * 128 x 128  -->    65 x 128 x 128 \n",
    "            nn.Conv2d(3,   64,  kernel_size=3,  padding=1 ),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # CL2: 64 x 128 x 128 -->    64 x 128 x 128\n",
    "            nn.Conv2d(64,   64,  kernel_size=3,  padding=1 ),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # MP1: 64 x 128 x 128 -->    64 x 64 x 64\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            # CL3: 64 x 64 x 64  -->    128 x 64 x 64 \n",
    "            nn.Conv2d(64,  128,  kernel_size=3,  padding=1 ),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # CL4: 128 x 64 x 64  -->    128 x 64 x 64 \n",
    "            nn.Conv2d(128,  128,  kernel_size=3,  padding=1 ),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # MP2: 128 x 64 x 64 -->    128 x 32 x 32\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            # CL5: 128 x 32 x 32  -->    256 x 32 x 32 \n",
    "            nn.Conv2d(128,256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # CL6: 256 x 32 x 32  -->    256 x 32 x 32     \n",
    "            nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # MP3: 256 x 32 x 32 -->    256 x 16 x 16\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            # CL7: 256 x 16 x 16  -->    512 x 16 x 16\n",
    "            nn.Conv2d(256,512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # CL8: 512 x 16 x 16  -->    512 x 16 x 16\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # MP4: 512 x 16 x 16 -->    512 x 8 x 8\n",
    "            nn.MaxPool2d(2,2)\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            # LL1:   512 x 8 x 8 = 32768 -->  4096 \n",
    "            nn.Linear(32768, 4096),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # LL2:   4096 -->  4096 \n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # LL3:   4096 -->  13\n",
    "            \n",
    "            nn.Linear(4096, 13),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer(x)\n",
    "        out = out.view(-1, 32768)\n",
    "        out = self.fc_layer(out)\n",
    "\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is running\n",
      "2061\n",
      "2061\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/clean_images_index\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device, \"is running\")\n",
    "\n",
    "images, labels = path_label_loader(path)\n",
    "print(len(images))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]),\n",
       " array([156, 152, 156, 170, 150, 129, 178, 141, 185, 159, 152, 161, 172]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 1648 images as training set and 413 images as test set\n",
    "train_indice = np.random.choice(2061, 1648, replace=False)\n",
    "train_path = images[train_indice]\n",
    "train_label = labels[train_indice]\n",
    "test_path = images[~train_indice]\n",
    "test_label = labels[~train_indice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_data_loader(train_path, train_label, 32)\n",
    "test_loader = create_data_loader(test_path, test_label, 32)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mlp=mlp(128*128*3, 50, 50, 13)\n",
    "my_mlp.to(device)\n",
    "print(my_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_record = train_model(net = my_mlp, n_epoch = 10, my_lr = 1e-4, train_loader = train_loader, test_loader = test_loader,is_cnn = False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lenet = LeNet5_convnet()\n",
    "my_lenet.to(device)\n",
    "print(my_lenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lenet_record = train_model(net = my_lenet, n_epoch = 10, my_lr = 1e-4, train_loader = train_loader, test_loader = test_loader, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG19(\n",
      "  (layer): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU()\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU()\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU()\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): ReLU()\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU()\n",
      "    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layer): Sequential(\n",
      "    (0): Linear(in_features=32768, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=4096, out_features=13, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "my_VGG16 = VGG16()\n",
    "my_VGG16.to(device)\n",
    "print(my_VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_212696/3433013089.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mVGG19_record\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_VGG19\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_212696/3520664265.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(net, n_epoch, my_lr, train_loader, test_loader, is_cnn, momentum, verbose)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mminibatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearn_course/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearn_course/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearn_course/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearn_course/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_212696/254334592.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageReadMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearn_course/lib/python3.7/site-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(path, mode)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \"\"\"\n\u001b[1;32m    259\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deeplearn_course/lib/python3.7/site-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36mdecode_image\u001b[0;34m(input, mode)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \"\"\"\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "VGG19_record = train_model(net = my_VGG19, n_epoch = 10, my_lr = 1e-4, train_loader = train_loader, test_loader = test_loader, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Backprop ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidedBackpropRelu(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        input = ctx.saved_tensors[0]\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[grad_input<0] = 0\n",
    "        grad_input[input<0]=0\n",
    "        return grad_input\n",
    "     \n",
    "guided_relu = GuidedBackpropRelu.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Backprop ReLU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidedReluModel(nn.Module):\n",
    "    def __init__(self,model,to_be_replaced,replace_to):\n",
    "        super(GuidedReluModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.to_be_replaced = to_be_replaced\n",
    "        self.replace_to = replace_to\n",
    "        self.layers=[]\n",
    "        self.output=[]\n",
    "        \n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m,self.to_be_replaced):\n",
    "                self.layers.append(self.replace_to )\n",
    "                #self.layers.append(m)\n",
    "            elif isinstance(m,nn.Conv2d):\n",
    "                self.layers.append(m)\n",
    "            elif isinstance(m,nn.BatchNorm2d):\n",
    "                self.layers.append(m)\n",
    "            elif isinstance(m,nn.Linear):\n",
    "                self.layers.append(m)\n",
    "            elif isinstance(m,nn.AvgPool2d):\n",
    "                self.layers.append(m)\n",
    "            elif isinstance(m,nn.MaxPool2d):\n",
    "                self.layers.append(m)\n",
    "                \n",
    "        for i in self.layers:\n",
    "            print(i)\n",
    "        \n",
    "        \n",
    "    def reset_output(self):\n",
    "        self.output = []\n",
    "    \n",
    "    def hook(self,grad):\n",
    "        # out = grad[:,0,:,:].cpu().data#.numpy()\n",
    "        out = grad[:,:,:,:].cpu().data#.numpy()\n",
    "        print(\"out_size:\",out.size())\n",
    "        self.output.append(out)\n",
    "        \n",
    "    def get_visual(self,idx,original_img):\n",
    "        grad = self.output[0][idx]\n",
    "        return grad\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = x \n",
    "        out.register_hook(self.hook)\n",
    "        for i in self.layers[:-3]:\n",
    "            out = i(out)\n",
    "        out = out.view(out.size()[0],-1)\n",
    "        for j in self.layers[-3:]:\n",
    "            out = j(out)\n",
    "        return out\n",
    "    \n",
    "    # def forward(self,x):\n",
    "    #     out = x \n",
    "    #     out.register_hook(self.hook)\n",
    "    #     for i in self.layers:\n",
    "    #         out = i(out)\n",
    "    #     out = out.view(out.size()[0],-1)\n",
    "    #     # out = out.view(-1, 102400)\n",
    "    #     return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def axes_swap(x):\n",
    "    '''Convert (3,n,n) to (n,n,3)'''\n",
    "    x = x.swapaxes(0,1)\n",
    "    x = x.swapaxes(1,2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Activation Map (CAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAM():\n",
    "    def __init__(self,model):\n",
    "        self.gradient = []\n",
    "        self.model = model\n",
    "        # self.h = self.model.model.layer[-2].register_backward_hook(self.save_gradient)\n",
    "        self.h = self.model.model.layer[0].register_backward_hook(self.save_gradient) ## Note the change of index here, the indexed layer should have grad?\n",
    "        \n",
    "    def save_gradient(self,*args):\n",
    "        #print(\"Gradient saved!!!!\")\n",
    "        grad_input = args[1]\n",
    "        grad_output= args[2]\n",
    "        self.gradient.append(grad_output[0])\n",
    "        print('CAM grad: ', self.gradient[0].size())\n",
    "        \n",
    "    def get_gradient(self):\n",
    "        return self.gradient[0]\n",
    "    \n",
    "    def remove_hook(self):\n",
    "        self.h.remove()\n",
    "            \n",
    "    def normalize_cam(self,x):\n",
    "        x = 2*(x-torch.min(x))/(torch.max(x)-torch.min(x)+1e-8)-1\n",
    "        #x[x<torch.max(x)]=-1\n",
    "        return x\n",
    "    \n",
    "    def visualize(self,cam_img,guided_img,img_var):\n",
    "        # guided_img = guided_img.sum(dim=0) # Compress RGB dimension? \n",
    "        guided_img = guided_img.numpy()\n",
    "        # cam_img = resize(cam_img.cpu().data.numpy(),output_shape=(28,28))\n",
    "        cam_img = cam_img.cpu().data.numpy()\n",
    "        x = img_var[:,:,:].cpu().data.numpy()\n",
    "        \n",
    "        x = axes_swap(x)\n",
    "        guided_img = axes_swap(guided_img)\n",
    "        # print('guided_img shape:',guided_img.shape)\n",
    "        guided_img = np.sum(guided_img, axis=-1)\n",
    "        # print('guided_img max:', guided_img.max())\n",
    "        # print('x_max:', x.max())\n",
    "        # print(x.shape)\n",
    "        \n",
    "        # x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        fig = plt.figure(figsize=(20, 12)) \n",
    "        \n",
    "        plt.subplot(1,4,1)\n",
    "        plt.title(\"Original Image\")\n",
    "        # plt.imshow(x,cmap=\"gray\")\n",
    "        plt.imshow(x)\n",
    "\n",
    "        plt.subplot(1,4,2)\n",
    "        plt.title(\"Class Activation Map\")\n",
    "        plt.imshow(cam_img)\n",
    "\n",
    "        plt.subplot(1,4,3)\n",
    "        plt.title(\"Guided Backpropagation\")\n",
    "        # plt.imshow(guided_img,cmap='gray')\n",
    "        plt.imshow(guided_img)\n",
    "        \n",
    "        plt.subplot(1,4,4)\n",
    "        plt.title(\"Guided x CAM\")\n",
    "        # plt.imshow(guided_img*cam_img,cmap=\"gray\")\n",
    "        global product\n",
    "        product = guided_img*cam_img\n",
    "        plt.imshow(guided_img*cam_img)\n",
    "        plt.show()\n",
    "    \n",
    "    def get_cam(self,idx):\n",
    "        grad = self.get_gradient()\n",
    "        print('grad shape:', grad.shape)\n",
    "        alpha = torch.sum(grad,dim=3,keepdim=True)\n",
    "        alpha = torch.sum(alpha,dim=2,keepdim=True)\n",
    "        print('alpha shape:', alpha.shape)\n",
    "        \n",
    "        cam = alpha[idx]*grad[idx]\n",
    "        cam = torch.sum(cam,dim=0) # Default Compression(of features?)\n",
    "        cam = self.normalize_cam(cam)\n",
    "        self.remove_hook()\n",
    "        \n",
    "        return cam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_lenet ## Change when best model is used\n",
    "guide = GuidedReluModel(model,nn.ReLU,guided_relu)\n",
    "cam = CAM(guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide.reset_output()\n",
    "for image,label in test_loader:\n",
    "    x = Variable(image,requires_grad=True).cuda()\n",
    "    x = x/255./4.\n",
    "    y_= Variable(label).cuda()\n",
    "        \n",
    "    output = guide.forward(x) \n",
    "    # print(output[0])\n",
    "    output = torch.index_select(output,dim=1,index=y_)\n",
    "    # print(output[0])\n",
    "    output = torch.sum(output)\n",
    "    # print(output)\n",
    "    output.backward(retain_graph=True)\n",
    "    \n",
    "    for j in range(1):\n",
    "        out = cam.get_cam(j)\n",
    "        guided_img = guide.get_visual(j,x)\n",
    "        cam.visualize(out,guided_img,x[j])\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(product.max())\n",
    "print(product.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unravel_index(product.argmax(), product.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product[61,124]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unravel_index(product.argmin(), product.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product[71, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product[71,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = axes_swap(x[j]).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(pic)\n",
    "\n",
    "# Create a Rectangle patch\n",
    "rect = patches.Rectangle(np.unravel_index(product.argmax(), product.shape), 10, 10, linewidth=1, edgecolor='b', facecolor='none')\n",
    "\n",
    "# Add the patch to the Axes\n",
    "ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
