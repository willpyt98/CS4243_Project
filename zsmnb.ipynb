{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "import imageio\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"..\\\\data\\\\clean_images_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_label_loader(path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "     path: Folder path containing subfolders of images\n",
    "    Output:\n",
    "     images: List of image path\n",
    "     labels: Numpy array of labels\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for s_folder in os.listdir(path):\n",
    "        label = int(''.join([s for s in s_folder if s.isdigit()]))\n",
    "        img_folder = os.path.join(path, s_folder)\n",
    "        \n",
    "        for img in os.listdir(img_folder):\n",
    "            if img.endswith(\".jpg\"):\n",
    "                image_path = os.path.join(img_folder, img)\n",
    "                images.append(image_path)\n",
    "                labels.append(label)\n",
    "                \n",
    "    labels = np.array(labels)\n",
    "                \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = path_label_loader(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2061\n",
      "2061\n"
     ]
    }
   ],
   "source": [
    "print(len(images))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13]),\n",
       " array([156, 152, 156, 170, 150, 129, 178, 141, 185, 159, 152, 161, 172],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test_set(verbose=1):\n",
    "    \n",
    "    test_data =  test_data.unsqueeze(dim=1)\n",
    "\n",
    "    # test_data=test_data.to(device)\n",
    "    # test_label=test_label.to(device)\n",
    "\n",
    "    inputs = (test_data - mean)/std    # ONLY CHANGE IS HERE!\n",
    "\n",
    "    scores=net( inputs ) \n",
    "\n",
    "    error = utils.get_error( scores , test_label)\n",
    "\n",
    "    error += error.item()\n",
    "\n",
    "\n",
    "    if verbose == 1:\n",
    "        print( 'error rate on test set =', total_error*100 ,'percent\\n')\n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, n_epoch, bs, lr, train_data, train_label, verbose=1):\n",
    "    \"\"\"\n",
    "    Train a given model with specified hyperparameters\n",
    "    \n",
    "    Args:\n",
    "     net: NN model to be trained\n",
    "     n_epoch: Number of epochs to train the model\n",
    "     bs: Batch size for minibatch GD\n",
    "     lr: Learning rate of GD\n",
    "     train_data: Torch tensor of dim [N:rgb:width:height]\n",
    "     train_label: Torch tensor of dim [N]\n",
    "     verbose: Print out metrics during training if 1, default 1\n",
    "    \n",
    "    Output:\n",
    "     net: Trained NN model\n",
    "     records: Dictionary containing metrics history, including training loss/error for each epoch/minibatch, test error for each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    N = train_data.shape[0]\n",
    "    \n",
    "    net = net.to(device)\n",
    "    mean = train_data.mean()\n",
    "    mean = mean.to(device)\n",
    "    std = train_data.std()\n",
    "    std = std.to(device)\n",
    "    \n",
    "    train_loss_hist_mb = []\n",
    "    train_loss_hist = []\n",
    "    train_error_hist = []\n",
    "    test_error_hist = []\n",
    "    \n",
    "    start=time.time()\n",
    "\n",
    "    for epoch in range(1,n_epoch+1):\n",
    "\n",
    "        if not epoch%5:\n",
    "            lr = lr / 1.5\n",
    "\n",
    "        optimizer=torch.optim.SGD( net.parameters() , lr=lr )\n",
    "\n",
    "        running_loss=0\n",
    "        running_error=0\n",
    "        num_batches=0\n",
    "\n",
    "        shuffled_indices=torch.randperm(N)\n",
    "\n",
    "        for count in range(0,N,bs):\n",
    "\n",
    "            # FORWARD AND BACKWARD PASS\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            indices=shuffled_indices[count:count+bs]\n",
    "            minibatch_data =  train_data[indices].unsqueeze(dim=1)\n",
    "            minibatch_label=  train_label[indices]\n",
    "\n",
    "            minibatch_data=minibatch_data.to(device)\n",
    "            minibatch_label=minibatch_label.to(device)\n",
    "\n",
    "            inputs = (minibatch_data - mean)/std      # ONLY CHANGE IS HERE!\n",
    "\n",
    "            inputs.requires_grad_()\n",
    "\n",
    "            scores=net( inputs ) \n",
    "\n",
    "            loss =  criterion( scores , minibatch_label) \n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            # COMPUTE STATS\n",
    "\n",
    "            running_loss += loss.detach().item()\n",
    "\n",
    "            error = utils.get_error( scores.detach() , minibatch_label)\n",
    "            running_error += error.item()\n",
    "\n",
    "            num_batches+=1   \n",
    "                        \n",
    "            train_loss_hist_mb.append(running_error/num_batches)\n",
    "\n",
    "\n",
    "        # AVERAGE STATS THEN DISPLAY\n",
    "        total_loss = running_loss/num_batches\n",
    "        total_error = running_error/num_batches\n",
    "        elapsed = (time.time()-start)/60\n",
    "\n",
    "        if verbose == 1:\n",
    "            print('epoch=',epoch, '\\t time=', elapsed,'min', '\\t lr=', my_lr  ,'\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "        test_error = eval_on_test_set(test_data, test_label, verbose=verbose) \n",
    "        \n",
    "        train_loss_hist.append(total_loss)\n",
    "        train_error_hist.append(total_error)\n",
    "        test_error_list.append(test_error)\n",
    "    \n",
    "    records = {'train_loss_mb': train_loss_hist_mb,\n",
    "              'train_loss': train_loss_hist,\n",
    "              'train_error': train_error_hist,\n",
    "              'test_error': test_error_list}\n",
    "        \n",
    "    return net, records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
