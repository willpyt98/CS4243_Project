{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "import imageio\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image,ImageReadMode\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../clean_images_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_label_loader(path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "     path: Folder path containing subfolders of images\n",
    "    Output:\n",
    "     images: List of image path\n",
    "     labels: Numpy array of labels\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for s_folder in os.listdir(path):\n",
    "        label = int(''.join([s for s in s_folder if s.isdigit()]))\n",
    "        img_folder = os.path.join(path, s_folder)\n",
    "        \n",
    "        for img in os.listdir(img_folder):\n",
    "            if img.endswith(\".jpg\"):\n",
    "                image_path = os.path.join(img_folder, img)\n",
    "                images.append(image_path)\n",
    "                labels.append(label)\n",
    "                \n",
    "    labels = np.array(labels)\n",
    "    images = np.array(images)\n",
    "                \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = path_label_loader(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2061\n",
      "2061\n"
     ]
    }
   ],
   "source": [
    "print(len(images))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13]),\n",
       " array([156, 152, 156, 170, 150, 129, 178, 141, 185, 159, 152, 161, 172],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir,labels, transform = False):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_dir)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_dir[idx]\n",
    "        img = read_image(img_path, ImageReadMode.RGB).float()\n",
    "        if self.transform != False:\n",
    "            img = self.transform(img)\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(img_path, img_label, batch_size):\n",
    "    transform = torch.nn.Sequential(\n",
    "        transforms.Resize(size = (128,128))\n",
    "    )\n",
    "    \n",
    "    dataset = CustomImageDataset(img_path, img_label, transform)\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test_set(test_loader, verbose=1):\n",
    "    \n",
    "    total_error = 0\n",
    "    batch_num = 0\n",
    "    \n",
    "    for test_data, test_label in test_loader:\n",
    "        \n",
    "        bs = test_label.shape[0]\n",
    "        test_data=test_data.to(device)\n",
    "        test_label=test_label.to(device)\n",
    "        inputs = test_data.view(bs, 128*128*3)\n",
    "        \n",
    "        scores=net( inputs ) \n",
    "        \n",
    "        test_label -= 1 # change this later\n",
    "\n",
    "        error = utils.get_error( scores , test_label)\n",
    "\n",
    "        total_error += error.item()\n",
    "        \n",
    "        batch_num += 1\n",
    "        \n",
    "    total_error = total_error / batch_num\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print( 'error rate on test set =', total_error*100 ,'percent\\n')\n",
    "        \n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, n_epoch, my_lr, train_loader, test_loader, verbose=1):\n",
    "    \"\"\"\n",
    "    Train a given model with specified hyperparameters\n",
    "    \n",
    "    Args:\n",
    "     net: NN model to be trained\n",
    "     n_epoch: Number of epochs to train the model\n",
    "     bs: Batch size for minibatch GD\n",
    "     lr: Learning rate of GD\n",
    "     train_data: Torch tensor of dim [N:rgb:width:height]\n",
    "     train_label: Torch tensor of dim [N]\n",
    "     verbose: Print out metrics during training if 1, default 1\n",
    "    \n",
    "    Output:\n",
    "     net: Trained NN model\n",
    "     records: Dictionary containing metrics history, including training loss/error for each epoch/minibatch, test error for each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    N = train_data.shape[0]\n",
    "    \n",
    "    net = net.to(device)\n",
    "    \n",
    "    train_loss_hist_mb = []\n",
    "    train_loss_hist = []\n",
    "    train_error_hist = []\n",
    "    test_error_hist = []\n",
    "    \n",
    "    start=time.time()\n",
    "\n",
    "    for epoch in range(1,n_epoch+1):\n",
    "\n",
    "        if not epoch%5:\n",
    "            my_lr = my_lr / 1.5\n",
    "\n",
    "        optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "\n",
    "        running_loss=0\n",
    "        running_error=0\n",
    "        num_batches=0\n",
    "\n",
    "        for minibatch_data, minibatch_label in train_loader:\n",
    "            \n",
    "\n",
    "            # FORWARD AND BACKWARD PASS\n",
    "            bs = minibatch_label.shape[0]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "\n",
    "            minibatch_data=minibatch_data.to(device)\n",
    "            minibatch_label=minibatch_label.to(device)\n",
    "            \n",
    "            minibatch_label -= 1 # Change this later \n",
    "            \n",
    "            inputs = minibatch_data.view(bs, 128*128*3)\n",
    "\n",
    "            inputs.requires_grad_()\n",
    "\n",
    "            scores=net( inputs ) \n",
    "\n",
    "            loss =  criterion( scores , minibatch_label.long()) \n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            # COMPUTE STATS\n",
    "\n",
    "            running_loss += loss.detach().item()\n",
    "\n",
    "            error = utils.get_error( scores.detach() , minibatch_label)\n",
    "            running_error += error.item()\n",
    "\n",
    "            num_batches+=1   \n",
    "                        \n",
    "            train_loss_hist_mb.append(running_error/num_batches)\n",
    "\n",
    "\n",
    "        # AVERAGE STATS THEN DISPLAY\n",
    "        total_loss = running_loss/num_batches\n",
    "        total_error = running_error/num_batches\n",
    "        elapsed = (time.time()-start)/60\n",
    "\n",
    "        if verbose == 1:\n",
    "            print('epoch=',epoch, '\\t time=', elapsed,'min', '\\t lr=', my_lr  ,'\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "        test_error = eval_on_test_set(test_loader, verbose=verbose) \n",
    "        \n",
    "        train_loss_hist.append(total_loss)\n",
    "        train_error_hist.append(total_error)\n",
    "        test_error_hist.append(test_error)\n",
    "    \n",
    "    records = {'train_loss_mb': train_loss_hist_mb,\n",
    "              'train_loss': train_loss_hist,\n",
    "              'train_error': train_error_hist,\n",
    "              'test_error': test_error_list}\n",
    "        \n",
    "    return net, records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2,  output_size):\n",
    "        super(mlp , self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(  input_size   , hidden_size1)\n",
    "        self.layer2 = nn.Linear(  hidden_size1 , hidden_size2)\n",
    "        self.layer3 = nn.Linear(  hidden_size2 , output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y       = self.layer1(x)\n",
    "        y_hat   = torch.relu(y)\n",
    "        z       = self.layer2(y_hat)\n",
    "        z_hat   = torch.relu(z)\n",
    "        scores  = self.layer3(z_hat)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp(\n",
      "  (layer1): Linear(in_features=49152, out_features=50, bias=True)\n",
      "  (layer2): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (layer3): Linear(in_features=50, out_features=13, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net=mlp(128*128*3, 50, 50, 13)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 1648 images as training set and 413 images as test set\n",
    "train_indice = np.random.choice(2061, 1648, replace=False)\n",
    "train_path = images[train_indice]\n",
    "train_label = labels[train_indice]\n",
    "test_path = images[~train_indice]\n",
    "test_label = labels[~train_indice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_data_loader(train_path, train_label, 64)\n",
    "test_loader = create_data_loader(test_path, test_label, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 1 \t time= 0.25191887219746906 min \t lr= 1e-05 \t loss= 2.5413213601479163 \t error= 91.66666659025046 percent\n",
      "error rate on test set = 91.78685889794276 percent\n",
      "\n",
      "epoch= 2 \t time= 0.7658549586931864 min \t lr= 1e-05 \t loss= 2.533039294756376 \t error= 91.54647428255814 percent\n",
      "error rate on test set = 91.92708340974954 percent\n",
      "\n",
      "epoch= 3 \t time= 1.2729599038759867 min \t lr= 1e-05 \t loss= 2.53014205969297 \t error= 91.86698725590338 percent\n",
      "error rate on test set = 91.64663461538461 percent\n",
      "\n",
      "epoch= 4 \t time= 1.7783260146776836 min \t lr= 1e-05 \t loss= 2.5255076518425574 \t error= 91.34615384615384 percent\n",
      "error rate on test set = 91.92708340974954 percent\n",
      "\n",
      "epoch= 5 \t time= 2.2877381404240924 min \t lr= 6.6666666666666675e-06 \t loss= 2.5192291736602783 \t error= 91.36618582101968 percent\n",
      "error rate on test set = 91.64663461538461 percent\n",
      "\n",
      "epoch= 6 \t time= 2.7963931798934936 min \t lr= 6.6666666666666675e-06 \t loss= 2.5157656027720523 \t error= 91.46634615384616 percent\n",
      "error rate on test set = 91.66666659025046 percent\n",
      "\n",
      "epoch= 7 \t time= 3.3136186480522154 min \t lr= 6.6666666666666675e-06 \t loss= 2.50998933498676 \t error= 91.18589735948123 percent\n",
      "error rate on test set = 91.44631417898032 percent\n",
      "\n",
      "epoch= 8 \t time= 3.8232190092404683 min \t lr= 6.6666666666666675e-06 \t loss= 2.5092452672811656 \t error= 90.92548076923077 percent\n",
      "error rate on test set = 91.64663461538461 percent\n",
      "\n",
      "epoch= 9 \t time= 4.337866282463073 min \t lr= 6.6666666666666675e-06 \t loss= 2.505039407656743 \t error= 91.2059295635957 percent\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3220\\3322742599.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmy_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3220\\3806489464.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(net, n_epoch, my_lr, train_loader, test_loader, verbose)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch='\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\t time='\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melapsed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\t lr='\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmy_lr\u001b[0m  \u001b[1;33m,\u001b[0m\u001b[1;34m'\\t loss='\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'\\t error='\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_error\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;34m'percent'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mtest_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_on_test_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mtrain_loss_hist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3220\\2913855274.py\u001b[0m in \u001b[0;36meval_on_test_set\u001b[1;34m(test_loader, verbose)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mbatch_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_label\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mbs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\Anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\Anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\Anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\Anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3220\\254334592.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImageReadMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\Anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\torchvision\\io\\image.py\u001b[0m in \u001b[0;36mread_image\u001b[1;34m(path, mode)\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimage_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \"\"\"\n\u001b[1;32m--> 259\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdecode_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\Anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\torchvision\\io\\image.py\u001b[0m in \u001b[0;36mread_file\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \"\"\"\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(net = net, n_epoch = 10, my_lr = 1e-5, train_loader = train_loader, test_loader = test_loader, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
