{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "import imageio\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image,ImageReadMode\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_label_loader(path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "     path: Folder path containing subfolders of images\n",
    "    Output:\n",
    "     images: List of image path\n",
    "     labels: Numpy array of labels\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for s_folder in os.listdir(path):\n",
    "        label = int(''.join([s for s in s_folder if s.isdigit()]))\n",
    "        img_folder = os.path.join(path, s_folder)\n",
    "        \n",
    "        for img in os.listdir(img_folder):\n",
    "            if img.endswith(\".jpg\"):\n",
    "                image_path = os.path.join(img_folder, img)\n",
    "                images.append(image_path)\n",
    "                labels.append(label)\n",
    "                \n",
    "    labels = np.array(labels) - 1\n",
    "    images = np.array(images)\n",
    "                \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir,labels, transform = False):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_dir)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_dir[idx]\n",
    "        img = read_image(img_path, ImageReadMode.RGB).float()\n",
    "        if self.transform != False:\n",
    "            img = self.transform(img)\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(img_path, img_label, batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size = (128,128)),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "        \n",
    "    \n",
    "    dataset = CustomImageDataset(img_path, img_label, transform)\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test_set(net, test_loader,is_cnn = True, verbose=1):\n",
    "    \n",
    "    total_error = 0\n",
    "    batch_num = 0\n",
    "    \n",
    "    for test_data, test_label in test_loader:\n",
    "        \n",
    "        bs = test_label.shape[0]\n",
    "        test_data=test_data.to(device)\n",
    "        test_label=test_label.to(device)\n",
    "        \n",
    "        if is_cnn:\n",
    "            inputs = test_data\n",
    "        else:\n",
    "            inputs = test_data.view(bs, 128*128*3)\n",
    "        \n",
    "        scores=net( inputs ) \n",
    "\n",
    "        error = utils.get_error( scores , test_label)\n",
    "\n",
    "        total_error += error.item()\n",
    "        \n",
    "        batch_num += 1\n",
    "        \n",
    "    total_error = total_error / batch_num\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print( 'error rate on test set =', total_error*100 ,'percent\\n')\n",
    "        \n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, n_epoch, my_lr, train_loader, test_loader, is_cnn = True, momentum=0.9, verbose=1):\n",
    "    \"\"\"\n",
    "    Train a given model with specified hyperparameters\n",
    "    \n",
    "    Args:\n",
    "     net: NN model to be trained\n",
    "     n_epoch: Number of epochs to train the model\n",
    "     bs: Batch size for minibatch GD\n",
    "     lr: Learning rate of GD\n",
    "     train_data: Torch tensor of dim [N:rgb:width:height]\n",
    "     train_label: Torch tensor of dim [N]\n",
    "     verbose: Print out metrics during training if 1, default 1\n",
    "    \n",
    "    Output:\n",
    "     net: Trained NN model\n",
    "     records: Dictionary containing metrics history, including training loss/error for each epoch/minibatch, test error for each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(train_loader)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    \n",
    "    train_loss_hist_mb = []\n",
    "    train_loss_hist = []\n",
    "    train_error_hist = []\n",
    "    test_error_hist = []\n",
    "    \n",
    "    start=time.time()\n",
    "\n",
    "    for epoch in range(1,n_epoch+1):\n",
    "\n",
    "        if not epoch%5:\n",
    "            my_lr = my_lr / 1.5\n",
    "\n",
    "        optimizer=torch.optim.SGD( net.parameters() , lr=my_lr, momentum=momentum )\n",
    "\n",
    "        running_loss=0\n",
    "        running_error=0\n",
    "        num_batches=0\n",
    "            \n",
    "        for minibatch_data, minibatch_label in train_loader:\n",
    "\n",
    "\n",
    "            # FORWARD AND BACKWARD PASS\n",
    "            bs = minibatch_label.shape[0]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            minibatch_data=minibatch_data.to(device)                \n",
    "            minibatch_label=minibatch_label.to(device)\n",
    "\n",
    "            if is_cnn:\n",
    "                inputs = minibatch_data\n",
    "            else:                    \n",
    "                inputs = minibatch_data.view(bs, 128*128*3)\n",
    "\n",
    "\n",
    "            inputs.requires_grad_()\n",
    "\n",
    "            scores=net( inputs ) \n",
    "\n",
    "            loss =  criterion( scores , minibatch_label.long()) \n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            # COMPUTE STATS\n",
    "            running_loss += loss.detach().item()\n",
    "\n",
    "            error = utils.get_error( scores.detach() , minibatch_label)\n",
    "            running_error += error.item()\n",
    "\n",
    "            num_batches+=1   \n",
    "            train_loss_hist_mb.append(running_error/num_batches)\n",
    "                \n",
    "        # AVERAGE STATS THEN DISPLAY\n",
    "        total_loss = running_loss/num_batches\n",
    "        total_error = running_error/num_batches\n",
    "        elapsed = (time.time()-start)/60\n",
    "\n",
    "        if verbose == 1:\n",
    "            print('epoch=',epoch, '\\t time=', elapsed,'min', '\\t lr=', my_lr  ,'\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "        test_error = eval_on_test_set(net, test_loader,is_cnn = is_cnn, verbose=verbose) \n",
    "        \n",
    "        train_loss_hist.append(total_loss)\n",
    "        train_error_hist.append(total_error)\n",
    "        test_error_hist.append(test_error)\n",
    "    \n",
    "    records = {'train_loss_mb': train_loss_hist_mb,\n",
    "              'train_loss': train_loss_hist,\n",
    "              'train_error': train_error_hist,\n",
    "              'test_error': test_error_hist}\n",
    "        \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2,  output_size):\n",
    "        super(mlp , self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(  input_size   , hidden_size1)\n",
    "        self.layer2 = nn.Linear(  hidden_size1 , hidden_size2)\n",
    "        self.layer3 = nn.Linear(  hidden_size2 , output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y       = self.layer1(x)\n",
    "        y_hat   = torch.relu(y)\n",
    "        z       = self.layer2(y_hat)\n",
    "        z_hat   = torch.relu(z)\n",
    "        scores  = self.layer3(z_hat)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5_convnet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(LeNet5_convnet, self).__init__()\n",
    "\n",
    "        # CL1:   3 * 128 x 128  -->    50 x 128 x 128 \n",
    "        self.conv1 = nn.Conv2d(3,   50,  kernel_size=3,  padding=1 )\n",
    "        \n",
    "        # MP1: 50 x 128 x 128 -->    50 x 64 x 64\n",
    "        self.pool1  = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        # CL2:   50 x 64 x 64  -->    100 x 64 x 64 \n",
    "        self.conv2 = nn.Conv2d(50,  100,  kernel_size=3,  padding=1 )\n",
    "        \n",
    "        # MP2: 100 x 64 x 64 -->    100 x 32 x 32\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        # LL1:   100 x 32 x 32 = 102400 -->  100 \n",
    "        self.linear1 = nn.Linear(102400, 100)\n",
    "        \n",
    "        # LL2:   100  -->  13 \n",
    "        self.linear2 = nn.Linear(100,13)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # CL1:   128 x 28  -->    50 x 28 x 28 \n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # MP1: 50 x 28 x 28 -->    50 x 14 x 14\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # CL2:   50 x 14 x 14  -->    100 x 14 x 14\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # MP2: 100 x 14 x 14 -->    100 x 7 x 7\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # LL1:   100 x 7 x 7 = 4900  -->  100 \n",
    "        x = x.view(-1, 102400)\n",
    "        x = self.linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # LL2:   4900  -->  10 \n",
    "        x = self.linear2(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is running\n",
      "2061\n",
      "2061\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/clean_images_index\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device, \"is running\")\n",
    "\n",
    "images, labels = path_label_loader(path)\n",
    "print(len(images))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]),\n",
       " array([156, 152, 156, 170, 150, 129, 178, 141, 185, 159, 152, 161, 172],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 1648 images as training set and 413 images as test set\n",
    "train_indice = np.random.choice(2061, 1648, replace=False)\n",
    "train_path = images[train_indice]\n",
    "train_label = labels[train_indice]\n",
    "test_path = images[~train_indice]\n",
    "test_label = labels[~train_indice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_data_loader(train_path, train_label, 32)\n",
    "test_loader = create_data_loader(test_path, test_label, 32)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp(\n",
      "  (layer1): Linear(in_features=49152, out_features=50, bias=True)\n",
      "  (layer2): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (layer3): Linear(in_features=50, out_features=13, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "my_mlp=mlp(128*128*3, 50, 50, 13)\n",
    "my_mlp.to(device)\n",
    "print(my_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 1 \t time= 0.10245367288589477 min \t lr= 0.0001 \t loss= 1722.2389654517174 \t error= 91.5264423076923 percent\n",
      "error rate on test set = 92.24759615384616 percent\n",
      "\n",
      "epoch= 2 \t time= 0.31244057019551597 min \t lr= 0.0001 \t loss= 2.7095618660633383 \t error= 92.42788461538461 percent\n",
      "error rate on test set = 92.0673076923077 percent\n",
      "\n",
      "epoch= 3 \t time= 0.5208546002705892 min \t lr= 0.0001 \t loss= 2.6140119433403015 \t error= 92.24759615384616 percent\n",
      "error rate on test set = 92.24759615384616 percent\n",
      "\n",
      "epoch= 4 \t time= 0.7284486095110575 min \t lr= 0.0001 \t loss= 2.5997384511507473 \t error= 92.36778846153845 percent\n",
      "error rate on test set = 92.12740384615384 percent\n",
      "\n",
      "epoch= 5 \t time= 0.9392736832300822 min \t lr= 6.666666666666667e-05 \t loss= 2.5911039664195132 \t error= 92.3076923076923 percent\n",
      "error rate on test set = 92.24759615384616 percent\n",
      "\n",
      "epoch= 6 \t time= 1.147477928797404 min \t lr= 6.666666666666667e-05 \t loss= 2.58593534047787 \t error= 92.42788461538461 percent\n",
      "error rate on test set = 92.0673076923077 percent\n",
      "\n",
      "epoch= 7 \t time= 1.357555083433787 min \t lr= 6.666666666666667e-05 \t loss= 2.582724772966825 \t error= 92.3076923076923 percent\n",
      "error rate on test set = 92.12740384615384 percent\n",
      "\n",
      "epoch= 8 \t time= 1.566036053498586 min \t lr= 6.666666666666667e-05 \t loss= 2.5811123114365797 \t error= 92.36778846153845 percent\n",
      "error rate on test set = 92.24759615384616 percent\n",
      "\n",
      "epoch= 9 \t time= 1.7733077685038248 min \t lr= 6.666666666666667e-05 \t loss= 2.578408406330989 \t error= 92.36778846153845 percent\n",
      "error rate on test set = 92.1875 percent\n",
      "\n",
      "epoch= 10 \t time= 1.9815388878186544 min \t lr= 4.4444444444444447e-05 \t loss= 2.5761287900117726 \t error= 92.36778846153845 percent\n",
      "error rate on test set = 92.12740384615384 percent\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp_record = train_model(net = my_mlp, n_epoch = 10, my_lr = 1e-4, train_loader = train_loader, test_loader = test_loader,is_cnn = False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5_convnet(\n",
      "  (conv1): Conv2d(3, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(50, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear1): Linear(in_features=102400, out_features=100, bias=True)\n",
      "  (linear2): Linear(in_features=100, out_features=13, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "my_lenet = LeNet5_convnet()\n",
    "my_lenet.to(device)\n",
    "print(my_lenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\Anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 1 \t time= 0.1593711256980896 min \t lr= 0.0001 \t loss= 49.306449321600105 \t error= 89.78365384615384 percent\n",
      "error rate on test set = 84.19471153846155 percent\n",
      "\n",
      "epoch= 2 \t time= 0.41911070346832274 min \t lr= 0.0001 \t loss= 2.182517700470411 \t error= 77.64423076923077 percent\n",
      "error rate on test set = 73.3173076923077 percent\n",
      "\n",
      "epoch= 3 \t time= 0.6817785064379375 min \t lr= 0.0001 \t loss= 1.8424647519221673 \t error= 62.5 percent\n",
      "error rate on test set = 44.230769230769226 percent\n",
      "\n",
      "epoch= 4 \t time= 0.944118070602417 min \t lr= 0.0001 \t loss= 1.3977575978407493 \t error= 44.17067307692308 percent\n",
      "error rate on test set = 42.78846153846153 percent\n",
      "\n",
      "epoch= 5 \t time= 1.2034389654795328 min \t lr= 6.666666666666667e-05 \t loss= 0.7536410322556129 \t error= 22.776442307692307 percent\n",
      "error rate on test set = 17.36778846153846 percent\n",
      "\n",
      "epoch= 6 \t time= 1.4674115935961405 min \t lr= 6.666666666666667e-05 \t loss= 0.4292052477025069 \t error= 11.23798076923077 percent\n",
      "error rate on test set = 13.100961538461538 percent\n",
      "\n",
      "epoch= 7 \t time= 1.7286170919736226 min \t lr= 6.666666666666667e-05 \t loss= 0.2310984074496306 \t error= 4.747596153846153 percent\n",
      "error rate on test set = 10.45673076923077 percent\n",
      "\n",
      "epoch= 8 \t time= 1.9924382328987122 min \t lr= 6.666666666666667e-05 \t loss= 0.12136870407714294 \t error= 2.2235576923076925 percent\n",
      "error rate on test set = 9.134615384615383 percent\n",
      "\n",
      "epoch= 9 \t time= 2.2560880541801454 min \t lr= 6.666666666666667e-05 \t loss= 0.08006798024647512 \t error= 1.0817307692307692 percent\n",
      "error rate on test set = 8.052884615384617 percent\n",
      "\n",
      "epoch= 10 \t time= 2.5192017515500384 min \t lr= 4.4444444444444447e-05 \t loss= 0.03375968455265348 \t error= 0.3004807692307693 percent\n",
      "error rate on test set = 8.11298076923077 percent\n",
      "\n",
      "epoch= 11 \t time= 2.7805936495463053 min \t lr= 4.4444444444444447e-05 \t loss= 0.023796263831452683 \t error= 0.06009615384615385 percent\n",
      "error rate on test set = 8.233173076923077 percent\n",
      "\n",
      "epoch= 12 \t time= 3.0401890238126117 min \t lr= 4.4444444444444447e-05 \t loss= 0.017062034851943072 \t error= 0.0 percent\n",
      "error rate on test set = 8.59375 percent\n",
      "\n",
      "epoch= 13 \t time= 3.305372865994771 min \t lr= 4.4444444444444447e-05 \t loss= 0.01360095828735771 \t error= 0.0 percent\n",
      "error rate on test set = 8.473557692307693 percent\n",
      "\n",
      "epoch= 14 \t time= 3.572603217760722 min \t lr= 4.4444444444444447e-05 \t loss= 0.011619446736473877 \t error= 0.0 percent\n",
      "error rate on test set = 8.353365384615383 percent\n",
      "\n",
      "epoch= 15 \t time= 3.8312930345535277 min \t lr= 2.962962962962963e-05 \t loss= 0.009582612224711249 \t error= 0.0 percent\n",
      "error rate on test set = 8.353365384615383 percent\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lenet_record = train_model(net = my_lenet, n_epoch = 15, my_lr = 1e-4, train_loader = train_loader, test_loader = test_loader, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
